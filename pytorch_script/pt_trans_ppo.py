# -*- coding: utf-8 -*-
"""pt_trans_PPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nDSbDyqvPZRngAoAF48JoN69wFfzSkqd

# Transformer goes to `gym`
In this notebook I am trying to make a simple transformer based bot to play different OpenAI gym game environments.

## Setup

In the first section of this notebook we setup our experimental configuration.
"""

# !pip3 install gym
# !pip3 install â€œgym[atari]
!pip3 install box2d-py

# Commented out IPython magic to ensure Python compatibility.
# import dep
import torch
import torch.nn as nn
from torch.distributions import Categorical
from torch.nn.parameter import Parameter
from torch.nn.utils import clip_grad_norm_

import time
from tqdm import tqdm
import copy
import numpy as np
import gym
import matplotlib.pyplot as plt
# %matplotlib inline
DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

ENV_NAME = 'Breakout-ram-v0'
print(DEVICE)

env = gym.make(ENV_NAME)
for i_episode in range(1):
    observation = env.reset()
    for t in range(0, 10, 5):
        #env.render()
        print(observation)
        action = env.action_space.sample()
        observation, reward, done, _ = env.step(action)
        print('*** ACTION:', action)
        print('*** REWARD:', reward)
        if done:
            print("Episode finished after {} timesteps".format(t+1))
            break

print(env.action_space.n)
env.close()

all_obs = []
env = gym.make(ENV_NAME)
for i_episode in range(1):
    observation = env.reset()
    for _ in range(300):
        action = env.action_space.sample()
        observation, reward, done, _ = env.step(action)
        all_obs.append(observation)
        if done:
            print("Episode finished after {} timesteps".format(t+1))
            break
env.close()

# make plot
plt.figure(figsize = (9, 9))
plt.imshow(all_obs, cmap=plt.get_cmap('bone'))
plt.title('state for 128 steps | ' + 'LunarLander-v2')
plt.xlabel('RAM locations')
plt.ylabel('Steps')

"""## Making the model
In this section we are using the same method and style that was used in `GPT-2` code written in tensorflow by the OpenAI team. However we are using `pytorch` instead of `tf` which can give us faster and easier experimentation setup. The network is mathematically called the policy, here we simplify the representation as $\pi_\theta(a|s)$ i.e. the policy is parametererised by parameters $\theta$ which takes input state $s$ and returns a probability distribution over all the actions $a$. The network has two heads, where one returns the policy and other returns the value of the state $v_\pi(s)$.
"""

"""
The complete model in form of function which can be called when required, the
PPO uses two different networks
"""

def gelu(x):
    out = 1 + torch.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * torch.pow(x, 3)))
    return out * x / 2


class LayerNorm(nn.Module):
    """construct layernorm module"""

    def __init__(self, n_state, epsilon=1e-5):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(n_state))
        self.beta = nn.Parameter(torch.zeros(n_state))
        self.epsilon = epsilon

    def forward(self, x):
        mean_ = x.mean((-1), keepdim = True)
        std = (x - mean_).pow(2).mean((-1), keepdim = True)
        x = (x - mean_) / torch.sqrt(std + self.epsilon)
        return self.gamma * x + self.beta

'''
class Conv1D(nn.Module):
    """1D convolution implementation in GPT"""

    def __init__(self, input_dim, output_dim):
        super(Conv1D, self).__init__()
        self.nf = output_dim
        # print(input_dim, output_dim)
        w = torch.empty(input_dim, output_dim)
        nn.init.normal_(w, std=0.2)
        self.w = Parameter(w)
        self.b = Parameter(torch.zeros(self.nf))
        
        # print('*** w.shape', self.w.shape)

    def forward(self, x):
        size_out = x.shape[:-1] + (self.nf,)
        
        # torch.addmm(beta=1, input, alpha=1, mat1, mat2, out=None)
        # out --> (beta * input) + alpha * matmul(mat1, mat2)
        print('**', x.shape)
        x_reshaped = x.view(x.shape[0] * x.shape[1], x.shape[-1])
        # print('** x_reshaped:', x_reshaped.shape)
        # print(x_reshaped.shape)
        # print(self.w.shape)
        # print(self.b.shape)
        out = torch.matmul(x_reshaped, self.w) + self.b
        x = out.view(*size_out)
        return x
'''

class Conv1D(nn.Module):
    def __init__(self, nx, nf):
        super(Conv1D, self).__init__()
        self.nf = nf
        w = torch.empty(nx, nf)
        nn.init.normal_(w, std=0.02)
        self.weight = Parameter(w)
        self.bias = Parameter(torch.zeros(nf))

    def forward(self, x):
        # print('***', x.size())
        size_out = x.size()[:-1] + (self.nf,)
        x = torch.addmm(self.bias, x.view(-1, x.size()[-1]), self.weight)
        return x.view(*size_out)


class Attention(nn.Module):
    """single block of attention"""

    def __init__(self, nx, n_ctx, config, scale=False):
        super(Attention, self).__init__()
        n_state = nx

        assert n_state % config.num_head == 0

        '''
        self.register_buffer() is used to register a buffer that is not used
        as a model parameter
        '''
        self.register_buffer('masking', torch.tril(torch.ones(n_ctx, n_ctx)). \
                             view(1, 1, n_ctx, n_ctx))
        self.num_head = config.num_head
        self.split_size = n_state
        self.scale = scale
        
        # three different conv pre-attention layers
        self.conv_attn = Conv1D(nx, n_state)
        self.conv_proj = Conv1D(n_state, nx)
        
        # self.attn_dropout = nn.Dropout(config.attn_pdrop)
        # self.resid_dropout = nn.Dropout(config.resid_pdrop)

    def _attn(self, q, k, v):
        w = torch.matmul(q, k)
        if self.scale:
            w /= np.sqrt(v.size(-1))
        nd, ns = w.size(-2), w.size(-1)
        b = self.masking[:, :, ns-nd:ns, :ns]
        w = w * b + 1e-9 * (1 - b)
        w = nn.Softmax(dim=-1)(w)
        return torch.matmul(w, v)

    def merge_heads(self, x):
        x = x.permute(0, 2, 1, 3).contiguous()
        new_shape = x.size()[:-2] + (x.size(-2) * x.size(-1),)
        return x.view(*new_shape)

    def split_heads(self, x, k=False):
        new_x_shape = x.size()[:-1] + (self.num_head, x.size(-1) // self.num_head)
        x = x.view(*new_x_shape)
        if k:
            return x.permute(0, 2, 3, 1)
        else:
            return x.permute(0, 2, 1, 3)

    def forward(self, query, key, value):
        '''x = self.conv_attn(x)
        try:
            query_, k_, v_ = x.split(self.split_size, dim=2)
        except ValueError as e:
            print('****', x.size())'''
        q1 = self.split_heads(self.conv_attn(query))
        k1 = self.split_heads(self.conv_attn(key), k=True)
        v1 = self.split_heads(self.conv_attn(value))
        
        # perform attention
        a = self._attn(q1, k1, v1)
        a = self.merge_heads(a)
        a = self.conv_proj(a)
        return a


class MLP(nn.Module):
    """MLP Module"""

    def __init__(self, hid_dim, config):
        super(MLP, self).__init__()
        nx = config.embedding_dim
        self.conv_fc = Conv1D(nx, hid_dim)
        self.conv_proj = Conv1D(hid_dim, nx)
        self.act = gelu

    def forward(self, x):
        out = self.act(self.conv_fc(x))
        out = self.conv_proj(out)
        return out


class Block(nn.Module):
    """One complete block / stack"""

    def __init__(self, n_ctx, config, scale=False):
        super(Block, self).__init__()
        nx = config.embedding_dim
        self.attn = Attention(nx, n_ctx, config, scale)
        self.ln1 = LayerNorm(nx)
        self.mlp = MLP(4 * nx, config)
        self.ln2 = LayerNorm(nx)

    def forward(self, x):
        out = self.attn(x, x, x)
        out = self.ln1(out + x)
        mlp_out = self.mlp(out)
        out = self.ln2(out + mlp_out)
        return out

class TransformerModel(nn.Module):
    """Transformer Model"""

    def __init__(self, config, n_ctx = 128):
        super(TransformerModel, self).__init__()
        self.n_ctx = n_ctx
        block = Block(n_ctx, config, scale=True)
        self.out = nn.ModuleList([copy.deepcopy(block) for _ in range(config.num_layers)])
        self.single_dim_proj = Conv1D(config.embedding_dim, 1)
        self.embed_proj = Conv1D(1, config.embedding_dim)

    def forward(self, x):
        # print(x.size)
        try:
            out = self.embed_proj(x)
        except RuntimeError:
            print('*** out.shape', x.size())
        # print(out.shape)
        # pass from the blocks
        for block in self.out:
            out = block(out)
            # print('**', out.shape)
        try:
            out = self.single_dim_proj(out).view(-1, self.n_ctx)
        except RuntimeError:
            print(out.size())
        return gelu(out)

class PolicyNetwork(nn.Module):
    """Transformer Model names after our lord of darkness StarScream (also
    Megtron is taken)"""

    def __init__(self, config, n_ctx):
        super(PolicyNetwork, self).__init__()
        self.out = TransformerModel(config, n_ctx)
        self.action_head = Conv1D(config.input_dim[0], config.num_actions[0])
        self.value_head = Conv1D(n_ctx, 1)
        self.acti_ = nn.Softmax(-1)
        
    def _process_obs(self, obs):
        if isinstance(obs, np.ndarray):
            state = torch.from_numpy(obs).float().to(DEVICE)
            state = state.unsqueeze(0)
            return state.unsqueeze(-1)/255.
        else:
            state = obs.detach()
            state = state.squeeze()
            return state.unsqueeze(-1)
        
    def act(self, obs, memory):
        state = self._process_obs(obs)
        # calculate values
        x = self.out(state)
        action_probs = self.acti_(self.action_head(x))
        dist = Categorical(action_probs)
        action = dist.sample()
        
        # add this to memory
        memory.states.append(state.detach())
        memory.actions.append(action)
        memory.logprobs.append(dist.log_prob(action))
        
        return action.item()
    
    def evaluate(self, obs, act):
        state = self._process_obs(obs)
        x = self.out(state) # features from the feature generator network
        
        # calculate action probabilities using action head
        action_probs = self.acti_(self.action_head(x))
        dist = Categorical(action_probs)
        action_logprobs = dist.log_prob(act)
        dist_entropy = dist.entropy()
        
        # calculate value of state
        state_value = self.value_head(x)
        return action_logprobs, torch.squeeze(state_value), dist_entropy

"""## Making the Policy Class

Remeber that unlike supervised learning where the architecture is everything, and we can place the loss function inside the same class only, we cannot do that same with RL. The policy in itself is a complete different thing that operates this network. The network is merely one half of the learner model, in fact there can be thousands of architecture inside each learner.

### Policy Gradients

In this sample code I am using PPO as our basic learning algorithm. PPO stands for Proximal policy optimization, as the name suggests this comes under the policy gradient algorithms. In policy gradient algorithms the agent tries to maximize the expectation of rewards, by targeting the policy directly. This is one of the simpler RL algorithms. It is especially useful because loss function depends upon the state distribution, which is stochastic and not static, thus cannot be put under derivation. Policy gradients solve it by breaking the learning operation in Bellman Equation.

$$
J(\theta) = \sum_{s \in S}d_\pi(s)V_\pi(s) = \sum_{s\in S}d_\pi(s) \sum_{a \in A} \pi_\theta(a|s) Q_\pi(s,a)
$$

where $d_\pi$ is the state distribution, which cannot be derivated upon and $V_\pi(s)$ is the value function depending on the parameters $\theta$. It can then further be expanded into an expectation of all the rewards from the actions. The policy $\pi$ returns a probability distribution of all the possible actions which are multiplied by the Q-value functions which return a value corresponding a being in state $s$ and taking action $a$. The derivative can then be calculated as follows:

$$
\nabla_\theta J(\theta) = \nabla_\theta \sum_{s\in S}d_\pi(s) \sum_{a \in A} \pi_\theta(a|s) Q_\pi(s,a) \\
\approx \sum_{s\in S}d_\pi(s) \sum_{a \in A} Q_\pi(s,a) \nabla_\theta \pi_\theta(a|s)
$$

My apologies but this is not a primer on the workings of RL, so I am not writing about the discounting, etc.

### Proximal Policy Optimization

PPO is step improvement in the same direction, where we first collect all the rewards that we get from playing an agent (thus requiring no discounting), then we compare it to what values were predicted by the learner. Becuase we already know what values were given to each state and we know the corresponding rewards, we can calculate the policy more efficiently. **PPO is an online learner, which performs well in both the discrete and continuous tasks.**  PPO is based on TRPO (trust region policy optimization) done by the same inventor John Schulman. TRPO increase training stability by introducing an advantage value which is diffrence between the Q-value $Q(s,a)$ and Value of the state $V(s)$. Using advantage the learner can learner gives more importance to places where it gave out different score, positive or otherwise.

In TRPO algorithm the larger advantage ($+$ or $-$) gives more importance to certain updates rather than the other. This is known as importance sampling. The objective is given as
$$
J(\theta) = \mathbb{E} \left[ \frac{\pi(a|s)}{\pi_{old}(a|s)}A_{\theta_{old}}(a,s)\right]
$$

TRPO aims to maximize the objective function $J(\theta)$ subject to, trust region constraint which enforces the distance between old and new policies measured by KL-divergence to be small enough, within a parameter $\delta$ as
$$
\mathbb{E}\left[ D_{KL}(\pi_{old}(a|s) \| \pi(a|s))\right] \leqslant \delta
$$

PPO introduce
"""

class PPO:
    def __init__(self, config):
        self.lr = config.lr
        self.gamma = config.gamma
        self.eps_clip = config.eps_clip
        self.k_epochs = config.k_epochs
        self.policy = PolicyNetwork(config, config.input_dim[0]).to(DEVICE)
        self.optimizer = torch.optim.Adam(self.policy.parameters(),
                                          lr = self.lr)
        self.policy_old = PolicyNetwork(config, config.input_dim[0]).to(DEVICE)
        self.mse_loss = nn.MSELoss()
        
    def update(self, memory):
        # monte carlo estimate of state rewards
        rewards = []
        dis_rew = 0
        for rew in reversed(memory.rewards):
            dis_rew = rew + (self.gamma * dis_rew)
            rewards.append(dis_rew)
        rewards = rewards[::-1]
        
        # normalizing the rewards
        rewards = torch.tensor(rewards).to(DEVICE)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)
        
        # convert list to tensor
        old_states = torch.stack(memory.states).to(DEVICE).detach()
        old_actions = torch.stack(memory.actions).to(DEVICE).detach()
        old_logprobs = torch.stack(memory.logprobs).to(DEVICE).detach()
        
        # print('*** old_states.size()', old_states.size())
        
        # optimize policy for k-epochs
        for _ in range(self.k_epochs):
            # evaluating the old states
            log_probs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)
            
            # find the ratio r(theta) = (pi_theta / pi_theta_old)
            ratios = torch.exp(log_probs / old_logprobs.detach())
            
            # finding surrogate loss
            advantages = rewards - state_values.detach()
            surr1 = ratios * advantages
            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages
            loss = -torch.min(surr1, surr2) + 0.5 * self.mse_loss(state_values, rewards) - 0.01 * dist_entropy
            
            # take a gradient step
            self.optimizer.zero_grad()
            loss.mean().backward()
            self.optimizer.step()
            
        # copy new weights into old policy
        self.policy_old.load_state_dict(self.policy.state_dict())

"""## Training the Learner

In this section we code up the parameters and tell the agent to go and learn.
"""

# definging the memory object, this is what it uses to learn
class Memory:
    def __init__(self):
        self.actions = []
        self.states = []
        self.logprobs = []
        self.rewards = []
    
    def clear_memory(self):
        del self.actions[:]
        del self.states[:]
        del self.logprobs[:]
        del self.rewards[:]

"""
NOTE: this is not a simple language model we use in SENTR and thus requires more
thoughtful designing. One thing is for sure that we will need more layers than
what we use for our internal SOTA.
"""
class ModelConfig:
    def __init__(self, in_dict):
        for k, v in in_dict.items():
            setattr(self, k, v)
            
    def add_config(self, name, value):
        setattr(self, name, value)

# environment
env_name = ENV_NAME
env = gym.make(env_name) # make env for values
observation_shape = list(observation.shape) + [1]
action_space = [env.action_space.n]

# artchitecture parameters
num_layers = 3
num_heads = 1
embedding_dim = 16 # each of the values is mapped to the 16 dims

# learner parameters
batch_size = 40
solved_reward = 230 # stop training is average reward is greater than solved reward
log_interval = 50 # print log every these many steps
max_episodes = int(5e+4) # max training episodes
max_timesteps = 300 # maximum time steps in one episode
update_timestep = 2000 # update policy every these many steps
lr = 0.002 # learning rate
gamma = 0.99 # discount factor
k_epochs = 4 # update policy every n timesteps
eps_clip = 0.2 # clip parameter for PPO
random_seed = 4

# make the model config object
config = ModelConfig(dict(
    env_name = env_name,
    
    num_layers = num_layers,
    num_head = num_heads,
    embedding_dim = embedding_dim,
    input_dim = observation_shape,
    num_actions = action_space,
    
    batch_size = batch_size,
    solved_reward = solved_reward,
    log_interval = log_interval,
    max_episodes = max_episodes,
    max_timesteps = max_timesteps,
    update_timestep = update_timestep,
    lr = lr,
    gamma = gamma,
    k_epochs = k_epochs,
    eps_clip = eps_clip,
    seed = random_seed
))

'''
for k, v in config.__dict__.items():
    if '__' not in k:
        print(k, v)
'''

np.random.seed(config.seed)
torch.manual_seed(config.seed)
env.seed(random_seed)

try:
    del memory
    del ppo
except:
    pass

# make memory and learners
memory = Memory()
ppo = PPO(config)

# logging variables
running_reward = 0
avg_length = 0
total_timesteps = 0
all_info = [] # at each iteration we store variety of values <rewards, curr_timestep>

timestep = 0
start_time = time.time()
print('====> STARTING TRAINING <====')
print('Max Episodes: {}. Likely to stop before that!'.format(config.max_episodes))
# run training iterations
for episode_idx in range(config.max_episodes):
    state = env.reset()
    ep_rewards = []
    for t in range(config.max_timesteps):
        timestep += 1
        total_timesteps += 1
        
        # running old policy
        action = ppo.policy_old.act(state, memory)
        state, reward, done, _ = env.step(action)
        memory.rewards.append(reward)
        ep_rewards.append(reward)
        
        # update if its time
        if timestep % update_timestep == 0:
            # we are having bug here
            print('updating params...')
            ppo.update(memory)
            memory.clear_memory()
            timestep = 0
            
        # running reward
        running_reward += reward
        if done:
            break

    avg_length += t
    all_info.append((ep_rewards, total_timesteps))
    
    # stop running if avg_reward > solved_reward
    if running_reward > (log_interval * solved_reward):
        print('=====> SOLVED! in {} sec <====='.format(time.time() - start_time))
        torch.save(ppo.policy.state_dict(), './PPO_{}.pth'.format(config.env_name))
        break
        
    # logging
    if episode_idx % log_interval == 0:
        avg_length = int(avg_length / log_interval)
        running_reward = int(running_reward / log_interval)
        
        print('** Episode: {}; Average Length: {}; Reward: {}'.format(episode_idx,
                                                                   avg_length,
                                                                   running_reward
                                                                  ))
        
        # reset
        running_reward = 0
        avg_length = 0

print('Number Updates: {}'.format(len(all_info)))

rew = []
for inf in all_info:
    rew.append(sum(inf[0]))
    
# plot results
plt.figure(figsize = (15, 5))
plt.plot(rew)
plt.title('Rewards vs. Episodes | {}'.format(config.env_name))

time.time()

